# Server Configuration

## PostgreSQL Connection Settings: Tuning for Performance and Accessibility

When deploying and managing a PostgreSQL server, understanding how clients connect—and how those connections are handled—forms the bedrock of performance, scalability, and security. Two of the most critical parameters under the umbrella of connection management are `max_connections` and `listen_addresses`. Though they may seem trivial at first glance, improper tuning can lead to major availability and security issues.

This section unpacks these settings, explaining what they do, how they impact real-world deployments, and how to configure them effectively for development, testing, and production workloads.

---

## `max_connections`: Defining How Many Clients Can Talk to Your Database

### What It Does

The `max_connections` setting in `postgresql.conf` determines the **maximum number of concurrent client connections** that PostgreSQL will allow at any one time. This includes connections from application servers, users, background jobs, and even some internal maintenance tasks.

**Default value:** `100`

This may sound like a lot, but it can quickly become insufficient in a busy environment—especially when each web or application server spawns multiple persistent or idle connections.

### Why It Matters

Setting `max_connections` too low can cause your application to throw “too many connections” errors under load. On the other hand, setting it too high without sufficient memory can crash the database server.

Each connection consumes **RAM** (around 5–10MB minimum, depending on configuration and workload). With too many active connections, you risk running out of memory or hitting OS-level process/thread limits.

Poorly managed connections are one of the **most common root causes of performance degradation** in PostgreSQL systems.

### Realistic Recommendations

| Server Type      | Suggested `max_connections`     |
| ---------------- | ------------------------------- |
| Small (dev/test) | 100–200                         |
| Medium           | 300–500                         |
| Large            | 1000+ (with connection pooling) |

> 🔒 **Note:** Never increase `max_connections` blindly. Ensure your server has the memory and CPU capacity to handle the potential load. Use connection pooling where possible.

### Check Your Current Usage

Before adjusting this value, it’s helpful to understand your system's current connection activity:

```sql
SELECT COUNT(*) FROM pg_stat_activity;
```

This query shows how many connections are currently active. You can also inspect individual sessions by running:

```sql
SELECT datname, usename, client_addr, state FROM pg_stat_activity;
```

---

### When to Use Connection Pooling

If your application opens lots of short-lived connections (e.g. web applications), PostgreSQL may spend more time managing connections than serving queries. In these scenarios, tools like **PgBouncer** or **Pgpool-II** are essential.

A connection pooler allows you to:

* **Reduce total database connections**
* **Reuse idle connections efficiently**
* **Smooth out spikes in demand**

With connection pooling in place, you can safely configure PostgreSQL to handle thousands of clients without exhausting system resources.

---

## `listen_addresses`: Who Is Allowed to Connect?

### What It Does

The `listen_addresses` setting controls which IP addresses PostgreSQL will listen on for incoming TCP/IP connections.

**Default value:** `'localhost'`

This means PostgreSQL only accepts connections from the same machine. It won’t be accessible to external applications or other machines on your network or cloud infrastructure.

### Why It Matters

If you're running PostgreSQL on your laptop for development or inside a secure on-prem server, the default value may be sufficient. However, if your application is deployed across multiple servers or containers, or if users need to connect remotely, you must explicitly configure PostgreSQL to accept external connections.

### Common Configurations

| Use Case                   | Recommended Setting                           |
| -------------------------- | --------------------------------------------- |
| Local-only development     | `listen_addresses = 'localhost'`              |
| Remote access (open IPs)   | `listen_addresses = '*'`                      |
| Remote access (restricted) | `listen_addresses = '192.168.0.10,localhost'` |

> ⚠️ **Important:** Setting `listen_addresses = '*'` allows PostgreSQL to listen on all network interfaces, which is risky if your server is exposed to the internet. Always combine this with proper firewalling and authentication.

---

### Checking the Current Setting

You can quickly inspect how your server is currently configured:

```sql
SHOW listen_addresses;
```

This is useful for troubleshooting remote connection issues.

---

### Don't Forget: pg\_hba.conf Also Matters

Even if `listen_addresses` is correctly configured, PostgreSQL will reject incoming connections unless your `pg_hba.conf` file allows them.

This file defines **host-based authentication** rules—i.e. who can connect, from where, and how.

For example, to allow password-based login from a specific IP range:

```conf
# pg_hba.conf
host    all             all             192.168.1.0/24        md5
```

Changes to `pg_hba.conf` and `listen_addresses` usually require a PostgreSQL **restart** or **reload**.

---

## Combining Both Settings for Real-World Scenarios

Here’s how you might configure both settings together based on your use case:

### 🧪 Development (Local Machine)

```conf
listen_addresses = 'localhost'
max_connections = 100
```

* No remote access
* Low number of concurrent users
* No connection pooling required

### 🌐 Staging or Internal Cloud App

```conf
listen_addresses = '0.0.0.0'
max_connections = 300
```

* Accessible by internal services or load balancers
* Suitable for testing load under production-like conditions
* Use pg\_hba.conf to limit access by IP range

### 🚀 Production (High Load with Pooling)

```conf
listen_addresses = '*'
max_connections = 1000
```

* PostgreSQL is accessed by many services, often via PgBouncer
* All external access controlled via firewall and `pg_hba.conf`
* Requires strong monitoring and alerting

---

## Tips for Safe Configuration Changes

* Always **back up** `postgresql.conf` and `pg_hba.conf` before making changes
* After editing config files, reload the configuration using:

```bash
sudo systemctl reload postgresql
```

* Or, from inside psql:

```sql
SELECT pg_reload_conf();
```

* Monitor your connection count regularly using `pg_stat_activity`
* If you’re unsure, use connection pooling rather than increasing `max_connections` too aggressively

---

## Final Thoughts

Connection settings may seem like a “set it and forget it” configuration, but they play a critical role in the scalability, security, and stability of your PostgreSQL environment. By understanding the relationship between `max_connections`, `listen_addresses`, connection pooling, and access control via `pg_hba.conf`, you can fine-tune your server to suit your workload—whether it’s a local development project or a globally distributed application.

In the next section, we’ll explore **authentication methods and encryption settings**, ensuring your database not only scales but does so securely.

---
Certainly! Below is a fully expanded **book-ready section** (approx. 1,100+ words) explaining the key PostgreSQL memory-related settings. This can fit into a broader chapter on performance tuning and system configuration.

---

# PostgreSQL Memory Settings: How to Optimise for Performance

PostgreSQL relies heavily on memory to deliver fast, consistent performance. Tuning its memory-related parameters correctly can dramatically reduce disk I/O, speed up query execution, and make better use of the operating system's caching mechanisms. However, PostgreSQL doesn’t automatically size itself based on your system RAM — you must manually configure most memory settings in the `postgresql.conf` file.

In this section, we’ll explore the most critical PostgreSQL memory settings: `shared_buffers`, `work_mem`, and `effective_cache_size`. Understanding how these work and how they interact with the operating system is essential for getting the most out of your PostgreSQL server.

---

## `shared_buffers`: PostgreSQL's Internal Data Cache

### What It Does

The `shared_buffers` parameter defines how much memory PostgreSQL allocates for its **internal buffer pool** — where it stores frequently accessed data blocks. This is PostgreSQL’s own cache, separate from the operating system’s file cache.

**Default value:** `128MB`

This default is far too low for production systems and is usually only suitable for development or small test environments.

### Why It Matters

If `shared_buffers` is too small:

* PostgreSQL will frequently access disk even for recently read data.
* Query performance will suffer, especially under concurrent load.

If `shared_buffers` is too large:

* PostgreSQL may compete with the operating system for RAM.
* You risk starving the OS of memory for file system caching and other essential operations.

The goal is to strike a balance between PostgreSQL's own caching and the operating system's file cache.

### Recommended Settings

| System Type          | Recommended Value           |
| -------------------- | --------------------------- |
| Development system   | 512MB – 1GB                 |
| Small server (8GB)   | 2GB                         |
| Medium server (16GB) | 4GB – 6GB                   |
| Large system (32GB+) | 8GB – 12GB (max 40% of RAM) |

As a general rule:

> Set `shared_buffers` to **25%–40%** of your system's total RAM.

For example, on a server with 16GB of RAM:

```conf
shared_buffers = 4GB
```

This gives PostgreSQL enough memory to cache active data sets while leaving room for the OS and other processes.

### Checking the Current Setting

To see what your server is currently using:

```sql
SHOW shared_buffers;
```

Remember that changes to `shared_buffers` require a **PostgreSQL restart** to take effect.

---

## `work_mem`: Memory Per Operation

### What It Does

The `work_mem` setting controls the **amount of memory allocated per sort or hash operation**. This is used during operations like:

* `ORDER BY`
* `DISTINCT`
* Hash joins
* Aggregations with GROUP BY

**Default value:** `4MB`

However, this is often too conservative for real-world workloads.

### Why It Matters

If `work_mem` is too low:

* PostgreSQL will spill intermediate results to disk (using temporary files).
* Queries that involve sorting or hashing will slow down significantly.

If `work_mem` is too high:

* You may run out of memory under heavy load, especially for complex queries or multiple concurrent sessions.
* Each connection could use multiple chunks of `work_mem` at once.

> 🧠 Important: `work_mem` is **per operation, per query** — not per session. A single complex query could consume multiple times the value of `work_mem`.

### Recommended Settings

| Workload Type          | Suggested Range               |
| ---------------------- | ----------------------------- |
| OLTP (transactional)   | 16MB – 64MB                   |
| OLAP (analytical)      | 64MB – 256MB or more          |
| Complex reporting jobs | Up to 512MB (carefully tuned) |

For example, to improve performance in a mixed workload:

```conf
work_mem = 64MB
```

This will improve performance for moderate sorting and joins, especially on datasets that would otherwise spill to disk.

### Checking Current Setting

To see your current `work_mem` configuration:

```sql
SHOW work_mem;
```

Or:

```sql
SELECT name, setting FROM pg_settings WHERE name = 'work_mem';
```

---

## `effective_cache_size`: Estimating Available OS Memory

### What It Does

The `effective_cache_size` setting does **not** allocate memory. Instead, it tells PostgreSQL **how much memory it can assume is available in the OS file system cache** for planning query execution.

This helps PostgreSQL make smarter decisions when choosing between different execution plans — for example, whether to use an index scan or a sequential scan.

**Default value:** `128MB`

This is far too low for any serious production workload.

### Why It Matters

PostgreSQL doesn’t directly control the OS-level file cache. However, it uses `effective_cache_size` to estimate whether certain operations are likely to be disk-bound or memory-resident.

If this value is **too low**:

* PostgreSQL assumes there is little caching available
* It may favour inefficient query plans, like full table scans

If this value is **too high**:

* PostgreSQL may be too optimistic and choose memory-intensive plans

You’re not allocating memory here — you're giving PostgreSQL an estimate.

### Recommended Settings

Set this to **50–75% of total system memory**, assuming your database server is dedicated.

| System RAM | Suggested `effective_cache_size` |
| ---------- | -------------------------------- |
| 8GB        | 4GB – 6GB                        |
| 16GB       | 8GB – 12GB                       |
| 32GB       | 16GB – 24GB                      |

Example setting for a server with 16GB of RAM:

```conf
effective_cache_size = 8GB
```

This helps PostgreSQL understand that a significant portion of its data might already be cached by the OS.

### Checking the Current Setting

You can inspect your current value with:

```sql
SHOW effective_cache_size;
```

Changes to this setting **do not require a restart** — they can be applied with a configuration reload.

---

## Putting It All Together

Here’s a practical example for a medium-sized production server with 16GB RAM:

```conf
# postgresql.conf
shared_buffers = 4GB
work_mem = 64MB
effective_cache_size = 8GB
```

This configuration strikes a balance between internal caching, query memory usage, and awareness of the OS cache, resulting in better query performance and fewer unnecessary disk reads.

---

## Best Practices and Tips

* Always monitor memory usage after making changes. Use tools like `top`, `htop`, or PostgreSQL’s own statistics views (`pg_stat_activity`, `pg_stat_statements`).
* Avoid setting `work_mem` too high in multi-tenant environments or when many sessions run in parallel.
* Use PostgreSQL extensions like `pg_stat_kcache` or external tools like `pgBadger` to analyse memory behaviour in real-world workloads.
* When in doubt, tune conservatively — under-allocating is safer than overcommitting memory in high-concurrency environments.

---

## Final Thoughts

PostgreSQL's default memory settings are intentionally conservative to ensure it starts safely on virtually any system. However, tuning these values to suit your hardware and workload is essential for unlocking real performance.

Understanding and adjusting `shared_buffers`, `work_mem`, and `effective_cache_size` allows PostgreSQL to cache data more effectively, avoid unnecessary disk I/O, and choose optimal execution plans. With just a few well-informed configuration changes, you can significantly improve throughput, latency, and scalability across your entire database infrastructure.

In the next section, we’ll explore **parallel query execution and planner behaviour**, giving you even more tools to fine-tune PostgreSQL for demanding workloads.



