# Server Configuration

## PostgreSQL Connection Settings: Tuning for Performance and Accessibility

When deploying and managing a PostgreSQL server, understanding how clients connect—and how those connections are handled—forms the bedrock of performance, scalability, and security. Two of the most critical parameters under the umbrella of connection management are `max_connections` and `listen_addresses`. Though they may seem trivial at first glance, improper tuning can lead to major availability and security issues.

This section unpacks these settings, explaining what they do, how they impact real-world deployments, and how to configure them effectively for development, testing, and production workloads.

---

## `max_connections`: Defining How Many Clients Can Talk to Your Database

### What It Does

The `max_connections` setting in `postgresql.conf` determines the **maximum number of concurrent client connections** that PostgreSQL will allow at any one time. This includes connections from application servers, users, background jobs, and even some internal maintenance tasks.

**Default value:** `100`

This may sound like a lot, but it can quickly become insufficient in a busy environment—especially when each web or application server spawns multiple persistent or idle connections.

### Why It Matters

Setting `max_connections` too low can cause your application to throw “too many connections” errors under load. On the other hand, setting it too high without sufficient memory can crash the database server.

Each connection consumes **RAM** (around 5–10MB minimum, depending on configuration and workload). With too many active connections, you risk running out of memory or hitting OS-level process/thread limits.

Poorly managed connections are one of the **most common root causes of performance degradation** in PostgreSQL systems.

### Realistic Recommendations

| Server Type      | Suggested `max_connections`     |
| ---------------- | ------------------------------- |
| Small (dev/test) | 100–200                         |
| Medium           | 300–500                         |
| Large            | 1000+ (with connection pooling) |

> 🔒 **Note:** Never increase `max_connections` blindly. Ensure your server has the memory and CPU capacity to handle the potential load. Use connection pooling where possible.

### Check Your Current Usage

Before adjusting this value, it’s helpful to understand your system's current connection activity:

```sql
SELECT COUNT(*) FROM pg_stat_activity;
```

This query shows how many connections are currently active. You can also inspect individual sessions by running:

```sql
SELECT datname, usename, client_addr, state FROM pg_stat_activity;
```

---

### When to Use Connection Pooling

If your application opens lots of short-lived connections (e.g. web applications), PostgreSQL may spend more time managing connections than serving queries. In these scenarios, tools like **PgBouncer** or **Pgpool-II** are essential.

A connection pooler allows you to:

* **Reduce total database connections**
* **Reuse idle connections efficiently**
* **Smooth out spikes in demand**

With connection pooling in place, you can safely configure PostgreSQL to handle thousands of clients without exhausting system resources.

---

## `listen_addresses`: Who Is Allowed to Connect?

### What It Does

The `listen_addresses` setting controls which IP addresses PostgreSQL will listen on for incoming TCP/IP connections.

**Default value:** `'localhost'`

This means PostgreSQL only accepts connections from the same machine. It won’t be accessible to external applications or other machines on your network or cloud infrastructure.

### Why It Matters

If you're running PostgreSQL on your laptop for development or inside a secure on-prem server, the default value may be sufficient. However, if your application is deployed across multiple servers or containers, or if users need to connect remotely, you must explicitly configure PostgreSQL to accept external connections.

### Common Configurations

| Use Case                   | Recommended Setting                           |
| -------------------------- | --------------------------------------------- |
| Local-only development     | `listen_addresses = 'localhost'`              |
| Remote access (open IPs)   | `listen_addresses = '*'`                      |
| Remote access (restricted) | `listen_addresses = '192.168.0.10,localhost'` |

> ⚠️ **Important:** Setting `listen_addresses = '*'` allows PostgreSQL to listen on all network interfaces, which is risky if your server is exposed to the internet. Always combine this with proper firewalling and authentication.

---

### Checking the Current Setting

You can quickly inspect how your server is currently configured:

```sql
SHOW listen_addresses;
```

This is useful for troubleshooting remote connection issues.

---

### Don't Forget: pg\_hba.conf Also Matters

Even if `listen_addresses` is correctly configured, PostgreSQL will reject incoming connections unless your `pg_hba.conf` file allows them.

This file defines **host-based authentication** rules—i.e. who can connect, from where, and how.

For example, to allow password-based login from a specific IP range:

```conf
# pg_hba.conf
host    all             all             192.168.1.0/24        md5
```

Changes to `pg_hba.conf` and `listen_addresses` usually require a PostgreSQL **restart** or **reload**.

---

## Combining Both Settings for Real-World Scenarios

Here’s how you might configure both settings together based on your use case:

### 🧪 Development (Local Machine)

```conf
listen_addresses = 'localhost'
max_connections = 100
```

* No remote access
* Low number of concurrent users
* No connection pooling required

### 🌐 Staging or Internal Cloud App

```conf
listen_addresses = '0.0.0.0'
max_connections = 300
```

* Accessible by internal services or load balancers
* Suitable for testing load under production-like conditions
* Use pg\_hba.conf to limit access by IP range

### 🚀 Production (High Load with Pooling)

```conf
listen_addresses = '*'
max_connections = 1000
```

* PostgreSQL is accessed by many services, often via PgBouncer
* All external access controlled via firewall and `pg_hba.conf`
* Requires strong monitoring and alerting

---

## Tips for Safe Configuration Changes

* Always **back up** `postgresql.conf` and `pg_hba.conf` before making changes
* After editing config files, reload the configuration using:

```bash
sudo systemctl reload postgresql
```

* Or, from inside psql:

```sql
SELECT pg_reload_conf();
```

* Monitor your connection count regularly using `pg_stat_activity`
* If you’re unsure, use connection pooling rather than increasing `max_connections` too aggressively

---

## Final Thoughts

Connection settings may seem like a “set it and forget it” configuration, but they play a critical role in the scalability, security, and stability of your PostgreSQL environment. By understanding the relationship between `max_connections`, `listen_addresses`, connection pooling, and access control via `pg_hba.conf`, you can fine-tune your server to suit your workload—whether it’s a local development project or a globally distributed application.

In the next section, we’ll explore **authentication methods and encryption settings**, ensuring your database not only scales but does so securely.

---
Certainly! Below is a fully expanded **book-ready section** (approx. 1,100+ words) explaining the key PostgreSQL memory-related settings. This can fit into a broader chapter on performance tuning and system configuration.

---

# PostgreSQL Memory Settings: How to Optimise for Performance

PostgreSQL relies heavily on memory to deliver fast, consistent performance. Tuning its memory-related parameters correctly can dramatically reduce disk I/O, speed up query execution, and make better use of the operating system's caching mechanisms. However, PostgreSQL doesn’t automatically size itself based on your system RAM — you must manually configure most memory settings in the `postgresql.conf` file.

In this section, we’ll explore the most critical PostgreSQL memory settings: `shared_buffers`, `work_mem`, and `effective_cache_size`. Understanding how these work and how they interact with the operating system is essential for getting the most out of your PostgreSQL server.

---

## `shared_buffers`: PostgreSQL's Internal Data Cache

### What It Does

The `shared_buffers` parameter defines how much memory PostgreSQL allocates for its **internal buffer pool** — where it stores frequently accessed data blocks. This is PostgreSQL’s own cache, separate from the operating system’s file cache.

**Default value:** `128MB`

This default is far too low for production systems and is usually only suitable for development or small test environments.

### Why It Matters

If `shared_buffers` is too small:

* PostgreSQL will frequently access disk even for recently read data.
* Query performance will suffer, especially under concurrent load.

If `shared_buffers` is too large:

* PostgreSQL may compete with the operating system for RAM.
* You risk starving the OS of memory for file system caching and other essential operations.

The goal is to strike a balance between PostgreSQL's own caching and the operating system's file cache.

### Recommended Settings

| System Type          | Recommended Value           |
| -------------------- | --------------------------- |
| Development system   | 512MB – 1GB                 |
| Small server (8GB)   | 2GB                         |
| Medium server (16GB) | 4GB – 6GB                   |
| Large system (32GB+) | 8GB – 12GB (max 40% of RAM) |

As a general rule:

> Set `shared_buffers` to **25%–40%** of your system's total RAM.

For example, on a server with 16GB of RAM:

```conf
shared_buffers = 4GB
```

This gives PostgreSQL enough memory to cache active data sets while leaving room for the OS and other processes.

### Checking the Current Setting

To see what your server is currently using:

```sql
SHOW shared_buffers;
```

Remember that changes to `shared_buffers` require a **PostgreSQL restart** to take effect.

---

## `work_mem`: Memory Per Operation

### What It Does

The `work_mem` setting controls the **amount of memory allocated per sort or hash operation**. This is used during operations like:

* `ORDER BY`
* `DISTINCT`
* Hash joins
* Aggregations with GROUP BY

**Default value:** `4MB`

However, this is often too conservative for real-world workloads.

### Why It Matters

If `work_mem` is too low:

* PostgreSQL will spill intermediate results to disk (using temporary files).
* Queries that involve sorting or hashing will slow down significantly.

If `work_mem` is too high:

* You may run out of memory under heavy load, especially for complex queries or multiple concurrent sessions.
* Each connection could use multiple chunks of `work_mem` at once.

> 🧠 Important: `work_mem` is **per operation, per query** — not per session. A single complex query could consume multiple times the value of `work_mem`.

### Recommended Settings

| Workload Type          | Suggested Range               |
| ---------------------- | ----------------------------- |
| OLTP (transactional)   | 16MB – 64MB                   |
| OLAP (analytical)      | 64MB – 256MB or more          |
| Complex reporting jobs | Up to 512MB (carefully tuned) |

For example, to improve performance in a mixed workload:

```conf
work_mem = 64MB
```

This will improve performance for moderate sorting and joins, especially on datasets that would otherwise spill to disk.

### Checking Current Setting

To see your current `work_mem` configuration:

```sql
SHOW work_mem;
```

Or:

```sql
SELECT name, setting FROM pg_settings WHERE name = 'work_mem';
```

---

## `effective_cache_size`: Estimating Available OS Memory

### What It Does

The `effective_cache_size` setting does **not** allocate memory. Instead, it tells PostgreSQL **how much memory it can assume is available in the OS file system cache** for planning query execution.

This helps PostgreSQL make smarter decisions when choosing between different execution plans — for example, whether to use an index scan or a sequential scan.

**Default value:** `128MB`

This is far too low for any serious production workload.

### Why It Matters

PostgreSQL doesn’t directly control the OS-level file cache. However, it uses `effective_cache_size` to estimate whether certain operations are likely to be disk-bound or memory-resident.

If this value is **too low**:

* PostgreSQL assumes there is little caching available
* It may favour inefficient query plans, like full table scans

If this value is **too high**:

* PostgreSQL may be too optimistic and choose memory-intensive plans

You’re not allocating memory here — you're giving PostgreSQL an estimate.

### Recommended Settings

Set this to **50–75% of total system memory**, assuming your database server is dedicated.

| System RAM | Suggested `effective_cache_size` |
| ---------- | -------------------------------- |
| 8GB        | 4GB – 6GB                        |
| 16GB       | 8GB – 12GB                       |
| 32GB       | 16GB – 24GB                      |

Example setting for a server with 16GB of RAM:

```conf
effective_cache_size = 8GB
```

This helps PostgreSQL understand that a significant portion of its data might already be cached by the OS.

### Checking the Current Setting

You can inspect your current value with:

```sql
SHOW effective_cache_size;
```

Changes to this setting **do not require a restart** — they can be applied with a configuration reload.

---

## Putting It All Together

Here’s a practical example for a medium-sized production server with 16GB RAM:

```conf
# postgresql.conf
shared_buffers = 4GB
work_mem = 64MB
effective_cache_size = 8GB
```

This configuration strikes a balance between internal caching, query memory usage, and awareness of the OS cache, resulting in better query performance and fewer unnecessary disk reads.

---

## Best Practices and Tips

* Always monitor memory usage after making changes. Use tools like `top`, `htop`, or PostgreSQL’s own statistics views (`pg_stat_activity`, `pg_stat_statements`).
* Avoid setting `work_mem` too high in multi-tenant environments or when many sessions run in parallel.
* Use PostgreSQL extensions like `pg_stat_kcache` or external tools like `pgBadger` to analyse memory behaviour in real-world workloads.
* When in doubt, tune conservatively — under-allocating is safer than overcommitting memory in high-concurrency environments.

---

## Final Thoughts

PostgreSQL's default memory settings are intentionally conservative to ensure it starts safely on virtually any system. However, tuning these values to suit your hardware and workload is essential for unlocking real performance.

Understanding and adjusting `shared_buffers`, `work_mem`, and `effective_cache_size` allows PostgreSQL to cache data more effectively, avoid unnecessary disk I/O, and choose optimal execution plans. With just a few well-informed configuration changes, you can significantly improve throughput, latency, and scalability across your entire database infrastructure.



# Logging and Monitoring in PostgreSQL: Diagnosing and Observing Performance

While PostgreSQL is a highly reliable and efficient database engine, real-world workloads inevitably introduce issues — whether that’s poor-performing queries, connection spikes, or failed transactions. To troubleshoot effectively, and to maintain insight into how PostgreSQL is behaving over time, **logging and monitoring** are critical.

In this section, we'll look at essential logging-related configuration parameters, especially `logging_collector` and `log_min_duration_statement`, and how they enable PostgreSQL administrators to observe, audit, and improve database performance.

---

## Why Logging Matters

Logging isn't just for debugging — it's for **observability**. In PostgreSQL, effective logging allows you to:

* Understand slow or blocking queries
* Trace errors and crashes
* Analyse patterns over time (e.g. spikes in query volume)
* Conduct security audits
* Feed logs into monitoring systems (e.g. Prometheus, ELK stack, pgBadger)

However, logging comes at a cost — both in terms of disk I/O and administrative overhead — so knowing which options to enable and how to configure them is key.

---

## `logging_collector`: Enabling PostgreSQL to Write Logs to Disk

### What It Does

The `logging_collector` parameter determines whether PostgreSQL writes logs to **physical log files on disk**, as opposed to relying solely on standard output (stdout), which may be captured by systemd or a container engine.

**Default value:** `off`

When disabled, logs may be lost or scattered unless redirected by the host system. Enabling this feature provides consistent, queryable logs for diagnostics and auditing.

### Why It Matters

With `logging_collector` turned on, PostgreSQL can:

* Store logs in structured, persistent files
* Enable advanced logging options like query timing
* Keep logs organised by day
* Allow integration with tools like `pgBadger`, ELK/EFK, or SIEMs

This is particularly useful in production environments, where log availability and audit trails are essential.

### Recommended Configuration

Here’s a sample configuration for typical production usage:

```conf
logging_collector = on
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d.log'
log_statement = 'none'
```

Let’s break this down:

* `logging_collector = on` enables logging to file
* `log_directory = 'pg_log'` defines the folder relative to the PostgreSQL data directory
* `log_filename` uses a daily timestamped pattern
* `log_statement = 'none'` is the safest default — more on this shortly

> 📝 **Note:** Ensure that the user running PostgreSQL has write permission to the log directory, and that your filesystem has enough space.

### Common Pitfall: Logging Everything

Setting `log_statement = 'all'` logs **every SQL statement**. While helpful for debugging or auditing, it can quickly:

* Consume large amounts of disk space
* Slow down query performance
* Flood logs with irrelevant noise (e.g. `SELECT 1` from monitoring)

Use this setting sparingly, or in controlled testing environments.

---

### Checking Log Configuration

To see where your logs are currently being written:

```sql
SHOW log_directory;
```

If you're unsure whether logs are enabled, you can check:

```sql
SHOW logging_collector;
```

Remember that enabling or changing `logging_collector` requires a **PostgreSQL restart** (not just reload).

---

## `log_min_duration_statement`: Tracking Slow Queries

### What It Does

The `log_min_duration_statement` setting controls whether PostgreSQL logs statements that take longer than a specified number of milliseconds to execute.

**Default value:** `-1` (disabled)

This is one of the **most useful logging options** for performance diagnostics.

### Why It Matters

By setting a threshold (e.g. 1,000 ms), you can log only the **slow queries** — the ones worth investigating — without overwhelming your log files with noise.

Slow query logging helps you:

* Identify inefficient SQL or missing indexes
* Spot queries affected by lock contention or I/O bottlenecks
* Monitor changes in application performance over time

This is especially important for OLTP systems, where query latency affects user experience.

### Recommended Settings

| Threshold                           | When to Use                                      |
| ----------------------------------- | ------------------------------------------------ |
| `log_min_duration_statement = 1000` | Log queries slower than 1s — typical baseline    |
| `log_min_duration_statement = 250`  | For performance tuning or dev/test environments  |
| `log_min_duration_statement = 0`    | Log **all** durations (be careful — high volume) |

In a production environment, the following is a sensible starting point:

```conf
log_min_duration_statement = 1000
```

This means: "Log any statement that takes more than 1 second to run."

> ⚠️ **Tip:** Avoid setting this value too low (e.g. 10ms) in production unless you're running short profiling sessions, as it can **flood your logs and impact disk I/O**.

---

### Analysing Slow Queries

Once you’ve enabled slow query logging, you can begin reviewing logs directly, or — even better — use the `pg_stat_statements` extension to gain insight into aggregate performance.

To find the top 5 slowest total execution times:

```sql
SELECT query, total_exec_time
FROM pg_stat_statements
ORDER BY total_exec_time DESC
LIMIT 5;
```

To use this feature, ensure the extension is loaded:

```sql
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
```

This approach gives you not only individual slow queries but also those that cumulatively cost your system the most.

---

## Best Practices for Logging and Monitoring

Here are some practical guidelines for effective PostgreSQL logging:

### ✅ Enable Core Logging

Start with:

```conf
logging_collector = on
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d.log'
log_min_duration_statement = 1000
```

This gives you:

* Daily logs
* Visibility into slow queries
* A basis for integrating with log analysis tools

### 🔍 Use Query Analysis Tools

Feed your logs into tools such as:

* **pgBadger** – a fast log parser with a web UI
* **ELK/EFK Stack** – centralised log analysis using Elasticsearch, Fluentd, and Kibana
* **Prometheus & Grafana** – paired with exporters like `postgres_exporter` for time-series metrics

### 🔐 Protect Log Files

Remember that logs may contain:

* SQL statements with sensitive data
* Connection and session metadata
* Error stack traces

Restrict file permissions and consider **log redaction or obfuscation** where appropriate.

### ⚙️ Rotate and Archive Logs

Use PostgreSQL's built-in log filename pattern for rotation:

```conf
log_filename = 'postgresql-%Y-%m-%d.log'
```

But also configure log file rotation and retention policies using tools like:

* `logrotate` (on Linux)
* Custom cron jobs
* Cloud-native logging agents (e.g. CloudWatch, Azure Monitor)

---

## Final Thoughts

Effective logging is the difference between flying blind and having actionable diagnostics. In PostgreSQL, enabling the right logging features — particularly `logging_collector` and `log_min_duration_statement` — allows you to stay ahead of performance issues, troubleshoot unexpected behaviour, and optimise resource usage.

By combining PostgreSQL's native logging with extensions like `pg_stat_statements` and external observability tools, you can build a powerful monitoring pipeline that gives you a clear view into how your database is running — and how it could run better.

In the next section, we’ll explore **PostgreSQL statistics views and wait event monitoring**, which allow even deeper insight into query execution and system bottlenecks.



